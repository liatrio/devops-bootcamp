# 7.2 Volumes

Kubernetes does not come with data persistence out of the box. For example, if a container in a pod crashes the kubelet will restart the container but with a clean state. The responsibility is on the developer to ensure that our applications have proper storage and data persistence. The [volume](https://kubernetes.io/docs/concepts/storage/volumes/) abstraction gives developers the ability to achieve those goals. At its core, a volume is a directory that stores data and is accessible to containers in a pod. There are three storage requirements that should be met: Storage should not depend on the pod lifecycle; Storage must be available on all nodes; Storage needs to survive even if a cluster crashes.

In this section we will be exploring three different volume abstractions: Persistent volumes, persistent volume claims, and storage classes.

## Persistent Volumes and Persistent Volume Claims

To help developers with storage and persisting data, Kubernetes offers us two API resources: Persistent volumes and persistent volume claims. A persistent volume is a cluster resource that is used to store data. PVs are plugins like volumes, but have a lifecycle independent from any pod that uses a PV. It's important to understand that PVs depend on external storage systems that are non-native to Kubernetes(NFS, EBS, CephFS, etc...) which allows for central management therefore decreasing the need for developers to have additional software and disk space on their systems. A Persistent volume claim is a request for storage by a user. Much like pods consume node resources, PVCs consume PV resources. PVs are resources in the cluster and PVCs are requests to use those resources.

The high level lifecycle for PVs and PVCs can be broken into a few parts. We first provision our PV, this can be done statically or dynamically. Next a user creates a PVC with the specified amount of storage. A control loop watches for new PVCs and if possible, matches a PV and binds them together.

> **Before jumping into the exercises we recommend reading a little bit about volumes, PVs, and PVCs** [Here](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)

## Exercise 1 - PVs and PVCs

In this exercise we will create a simple PV and PVC.

1. Use Docker Desktop for your cluster.
2. Clone the [bootcamp repository](https://github.com/liatrio/devops-bootcamp.git) if you haven't already and apply the simple PV definition yaml example from `examples/ch7/volumes/pv-definition.yaml`

  !> *This PV utilizes a `hostPath` field, which should not be used in production as it poses security risks. It is acceptable for demonstration purposes.*

3. Use the command `kubectl get pv` to list your persistentVolumes. *Remember that PVs are cluster scoped, therefore they do not belong to a particular namespace*

4. Now that you have created a PV, create a PVC that will bind to the PV you just created.

?> Hint: Look at the Kubernetes documentation on pvcs

Now that you have created both a persistent volume and a persistent volume claim, let's look at how to utilize them.

Here is a pod definition yaml that uses an alpine image, generates a random number between 1-100 and saves the result to a file.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myPod
spec:
  containers:
    - name: alpine
      image: alpine
      command: ["/bin/sh", "-c"]
      args: ["shuf -i 0-100 -n 1 >> /opt/number.out;"]
```

5. Apply this file with `kubectl apply -f examples/ch7/volumes/random-num-pod.yaml`

?> Note: We create our pod definition declaratively. What would deploying this pod using an imperative command look like? If there is time see if you can recreate this pod without a pod-definition.yaml

6. As we can see, if this pod dies or the container needs to be recreated we will lose the file with our randomly generated number. Add the necessary fields under the spec for this pod to mount the data to our previously created PVC.

## Exercise 2 - Dynamic Provisioning

So far we have seen how to provision a persistent volume statically. However, this can become tedious and as a cluster administrator, you would have to manually provision hundreds of these for teams of developers in an organization. Instead, we can provision PVs dynamically, allowing storage volumes to be created on demand. To achieve this a cluster admin can define as many `StorageClass` objects as needed.

By now you are familiar with Jenkins. However, in previous exercises you were using Jenkins for demonstration purposes and didn't need to worry about persisting data. What if we needed to provision Jenkins instances in our cluster and ensure that if the pod crashed or the container re-created we wouldn't lose our Jenkins user? This is another excellent use case for PVs and PVCs.

1. Using Docker Desktop create a new namespace `jenkins`

2. Docker Desktop has a default StorageClass `docker.io/hostpath` that we will be using. Use the command `kubectl get storageclass` to see it. *StorageClasses are cluster scoped so you do not need to supply a namespace for this command*.

3. As a cluster administrator, you need to pre-create a storage class object that developers can use. Create a StorageClass definition yaml that has the provisioner `docker.io/hostpath`, a volumeBindingMode `immediate` and a name of `standard`. Apply your StorageClass definition file to your cluster with `kubectl apply -f storage-class-definition.yaml`.

?> Note: Creating the storage class object allows the cluster admin to dictate what storage classes are allowed by the cluster. If you tried to dynamically provision a pvc with a storage class other than `docker.io/hostpath` the cluster would reject it.

4. Next, create a pvc that will request resources dynamically. Make sure to use the storage class made in the previous step. Name your pvc `pvc-jenkins`. Apply your pvc definition to your cluster with `kubectl apply -f pvc-definition.yaml`.

5. To make sure we dynamically provisioned a persistent volume, use the command `kubectl get pv` to confirm it was created and is bound to our pvc.

6. Next we will use a Kubernetes deployment to deploy our Jenkins pod. Navigate to the Jenkins deployment definition in the folder `examples/ch7/volumes/jenkins-deployment.yaml`.

7. Add the required fields to our deployment definition that mounts the data to the previously created pvc. The fields are exactly the same as a pod for adding volume mounts to pvcs.

?> Hint: The `mountPath` field should be set to `/var/jenkins_home`

8. Apply your deployment to the cluster.

9. We need a service to expose our Jenkins deployment. Apply the service definition configuration at `examples/ch7/volumes/jenkins-services.yaml`.

10. Port-forward the `jenkins` service and access Jenkins through localhost.

11. Look into the logs for the Jenkins pod in our Jenkins namespace to find the initial Administrator Password.

12. Make sure you can access Jenkins and go through the setup process.

13. Delete the Jenkins pod. Does the data persist when it is redeployed? You might have to port-forward the service again.

## Deliverables

- Discuss the importance of having data persistence.
- What is the difference between persistent volumes and persistent volume claims?
- What are the advantages of dynamic provisioning as opposed to static provisioning of pvs?
- How do storage classes relate to pvs and pvcs?
