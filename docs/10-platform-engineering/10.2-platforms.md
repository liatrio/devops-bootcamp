---
docs/10-platform-engineering/10.2-platforms.md:
  category: Platform Engineering
  estReadingMinutes: 15
  exercises:
    - name: Create a TVP (Thinnest Viable Platform)
      description: |
        Students will use Terraform, Terragrunt and a library/deploy repo structure to create a core EKS platform with ArgoCD and External Secrets Operators
      estMinutes: 3000
      technologies:
        - AWS
        - Terraform
        - Terragrunt
        - Kubernetes
        - ArgoCD
        - ExternalSecrets
---

# Platforms

Backstage is a powerful Internal Developer Platform that excels at software discoverability and accelerates time to value and security adoption through its software templates.
However, Backstage doesn't provide the runtime environment for your applications. In modern software development, we increasingly need to run containerized applications.
Kubernetes has emerged as the de facto standard for container orchestration, but should every developer be expected to be a Kubernetes administrator? The discipline of Platform Engineering addresses this by providing developers
with a centralized platform solution. This manifests as a well-maintained Kubernetes cluster with common tools that streamline the running of applications in a secure, centralized environment.

## TVP

For this exercise, we will create the Thinnest Viable Platform (TVP). This will be an EKS cluster with ArgoCD, Metrics Server installed and configured, AWS LoadBalancer Controller, and ExternalSecrets operator installed and configured.
The end goal of this exercise is to create a simple platform and then onboard a new application to the platform via a Backstage template. This demonstrates a streamlined
approach to getting developers up and running in Kubernetes and provides a framework for Platform teams to build in sensible defaults.

The architecture for this platform will be split into a library and a deploy repo. The library repo will contain Terraform and Argo Application manifests
that install and configure individual components of the platform, each independently versioned. The deploy repo will contain Terragrunt to orchestrate and deploy
your versioned components.

![tvp architecture](./img10/tvp-arch.png ':class=img-center :alt=TVP Architecture')

### Library Repo

This repo will hold versioned Terraform modules and Kubernetes manifests that form the building blocks of our platform. These are split from the deploy repo so that platform teams can develop these components separately without affecting production environments. While ArgoCD will manage most Kubernetes applications, some foundational components (like ArgoCD itself) need to be bootstrapped with Terraform. Each component is independently versioned, allowing teams to update and test components without impacting the entire platform.

Your library repo should contain well-documented, reusable modules and manifests that follow infrastructure-as-code best practices. Teams can reference specific versions of these components, ensuring consistent and reproducible deployments across environments.

```bash
.
├── .github
│   └── workflows
│       ├── pr-lint.yml      # Lint, format and validate Terraform and Kubernetes manifests
│       └── release.yml      # Semantically release modules and applications
├── .gitignore
├── README.md
├── modules
│   ├── argocd
│   │   ├── README.md
│   │   ├── main.tf
│   │   ├── variables.tf
│   │   └── versions.tf
│   ├── eks
│   │   ├── main.tf
│   │   ├── outputs.tf
│   │   ├── variables.tf
│   │   └── versions.tf
│   └── vpc
│       ├── main.tf
│       ├── outputs.tf
│       ├── variables.tf
│       └── versions.tf
├── applications
│   ├── metrics-server
│   │   ├── kustomization.yaml
│   │   └── metrics-server.yaml
│   ├── external-secrets
│   │   ├── kustomization.yaml
│   │   └── external-secrets.yaml
│   └── aws-load-balancer-controller
│       ├── kustomization.yaml
│       └── aws-load-balancer-controller.yaml
```

Your Terraform should be linted, formatted, and validated. Merging to main should create semantic versioned tags for each module in this library repo. For instance,
you should have tags like `eks/v0.1.0` and `metrics-server/v0.0.1`. While specific naming conventions are flexible, the principle is that modules should be versioned
independently. We don't want a change merged to the `vpc` module causing a version bump on the `argocd` module.

?> The [Hashicorp EKS tutorial](https://developer.hashicorp.com/terraform/tutorials/kubernetes/eks) is a good starting point for modeling your modules

### Deploy Repo

This repo orchestrates the deployment of your platform by composing versioned components from the library repo. Using Terragrunt, a lightweight wrapper around Terraform, we manage infrastructure deployments with improved code reuse and reduced configuration complexity. The separation between library and deploy repos creates a clean distinction between defining components and implementing them, allowing platform teams to maintain consistent standards while staying flexible.

The deploy repo structure separates infrastructure from applications, making it clear where and how components are deployed. Infrastructure is managed through Terragrunt, while applications are deployed using Kustomize configurations that reference specific versions from the library repo. This approach provides a streamlined path for both platform engineers managing infrastructure and application teams deploying workloads.

```bash
.
├── .github
│   └── workflows
│       ├── apply.yaml
│       └── pr.yaml
├── .gitignore
├── README.md
├── bootstrap # Folder for terraform that will create required roles, policies, and identity providers needed by CI (CI should ignore this directory)
│   ├── README.md
│   ├── main.tf
│   ├── outputs.tf
│   └── variables.tf
├── common-inputs.yaml # Place for common variables like aws_region, which can be used in all terraform/terragrunt configuration
├── prod
│   ├── infra
│   │   ├── eks
│   │   │   └── terragrunt.hcl
│   │   ├── vpc
│   │   │   └── terragrunt.hcl
│   │   └── argocd
│   │       └── terragrunt.hcl
│   └── applications
│       ├── metrics-server
│       │   └── application.yaml  # Argo Application referencing the deploy repo
│       │   └── kustomization.yaml  # References metrics-server from library repo
│       └── hello-world
│           └── kustomization.yaml  # References app team's repository
│           └── application.yaml  # Argo Application referencing the deploy repo
└── terragrunt.hcl
```

## Exercise 1

Before we can onboard an application we need to create our TVP. We will start by setting up our repos and building out a
repeatable EKS cluster.
- Create the deploy repo:
  - Locally create the bootstrap infra required to have an OIDC ready role for managing infra in your AWS account.
  - Create GitHub Actions for PR plan and Apply of environment infrastructure.
- Create the library repo:
  - Create GitHub Actions that will lint, format, and enforce conventional commits. Merges to main should
  create versioned tags of your modules.
  - Create a VPC module
  - Create an EKS module
- Using Terragrunt orchestrate and deploy your VPC and EKS module in the deploy repo
- Verify that everyone can configure their kubeconfig and authenticate to the EKS module

?> Make both the deploy and the library repo private. This is just out of an abundance of caution as you will have workflows that will likely have very elevated permissions.

## Exercise 2

![ArgoCD Logo](https://argo-cd.readthedocs.io/en/stable/assets/logo.png ':class=img-center :alt=TVP Architecture')

Wonderful now you have an EKS cluster stood up with IaC. Now we are going to add ArgoCD to the cluster as our preferred method for deploying Kubernetes applications.
First off what is ArgoCD? ArgoCD is a GitOps controller that manages Kubernetes resources by treating Git as the single source of truth. Unlike traditional CI/CD push-based deployments,
ArgoCD uses a pull-based model where it runs inside your cluster and continuously monitors your Git repositories for changes. When it detects drift between the desired state
in Git and the actual state in the cluster, it automatically reconciles the differences. This approach offers several key advantages: enhanced security since no external
system needs direct cluster access, better auditability as all changes are tracked in Git, and improved reliability through constant state reconciliation.
ArgoCD also provides a powerful UI for visualizing your applications' deployment status and health, supports multiple config management tools like Helm and Kustomize,
and can manage applications across multiple clusters from a single interface. Its declarative nature aligns perfectly with Kubernetes' own paradigms, making it a natural fit for cloud-native deployments.

It's also worth knowing about the [Rendered Manifests Pattern](https://akuity.io/blog/the-rendered-manifests-pattern). Instead of having ArgoCD render Helm or Kustomize templates at deploy time, you render them in CI and commit the plain YAML to Git. This makes pull requests easier to review since you see the actual manifests, not just template changes. It also catches templating errors earlier. The downside is an extra CI step, but many teams prefer this tradeoff.

- Create and install ArgoCD into the cluster through a Terrafrom module (write this in the Library repo)
- Actually Deploy and ArgoCD into the cluster via the Deploy repo with Terragrunt
- Verify that all of you can access the ArgoCD UI by port forwarding the `argocd-service`

?> Default admin password is stored as a Kubernetes secret on the cluster

?> As you add more applications, consider the [App of Apps pattern](https://argo-cd.readthedocs.io/en/stable/operator-manual/cluster-bootstrapping/#app-of-apps-pattern) to manage them hierarchically from a single root Application.

## Exercise 3

Now that we have stood up ArgoCD we can let Argo manage our Kubernetes applications. There are many core applications that all
application teams are going to want to have access. These applications generally fall to the platform teams. Some examples of these are:
- Metrics Server
- OTEL Operators & Collectors
- AWS Load Balancer Controller
- Cert-Manager
- ExternalDNS
- Kyverno/OPA Gatekeeper
- External Secrets Operator
- Karpenter
etc

The list is long but the thing to note here is that these are all Kubernetes applications that are deployed ultimately
via manifest files (Helm charts just result in manifest files). So given our cluster now has a Kubernetes GitOps Controller
installed and configured we are going to let ArgoCD own the deployment of these Kubernetes applications.

- Enable IRSA (IAM Roles for Service Accounts) on your EKS cluster. IRSA lets pods authenticate to AWS services without storing long-lived credentials. It works by linking a Kubernetes service account to an IAM role via an OIDC provider. When a pod uses that service account, it receives temporary AWS credentials automatically. You'll need this for AWS Load Balancer Controller and External Secrets Operator.
- In your library repo create a new directory `applications/metrics-server`
  - Using what you have learned add file(s) that will source manifests from a pinned version of the [official repo for metrics-server](https://github.com/kubernetes-sigs/metrics-server)
  > Verify that pushing the metrics-server application to your library repo generates a tagged version in the library repo
- In your deploy repo create a new directory `applications/metrics-server` under your environment (ie: `prod/applications/metrics-server`)
- Use Kustomize to reference your versioned application in the library repo
- Next create an Argo Application manifest that will point to your deploy repo at `applications/metrics-server`
- Add a new workflow to your deploy repo that will apply the Argo Application manifests
- Verify in the Argo UI that metrics-server was picked up, synced, and healthy.
- Try changing the version of metrics server that you have pinned in your library repo and roll that change out the the cluster.

?> [ArgoCD's Credential Templates](https://argo-cd.readthedocs.io/en/stable/user-guide/private-repositories/#credential-templates) are an easy way to authenticate to private/internal repos

## Exercise 4

We have the framework for a platform now we need a way for teams to use our platform. In this exercise we will create a backstage template that
injects a new Argo Application into our Deploy repo that references an external manifests repo.
- Create a simple Hello World NodeJS application and containerize it. Create workflows that semantically release, build, and publish the resulting image to a contrainer registry
  > Make this repo and the container registry public
- Create a manifest repo that contains only the required k8s manifests to run your hello world application (likely just a Deployment and a Service)
- Create a backstage template to onboard an 'app teams' application. Your template should do the following.
  - Take in the repo, version, and application name
  - Create a PR to your deploy repo that adds `prod/applications/<app-name>/kustomization.yaml` and `prod/applications/<app-name>/application.yaml`
    > Your application.yaml should point to `prod/applications/<app-name>` at HEAD in your deploy repo. Your kustomization.yaml should point to
    to the app teams repo at the version specified in the template.
- Ensure that the person that onboarded the app is reflected in the Pull Request (PR Author, or otherwise)
- Review and merge the pull request
- Verify the application was successfully deployed to the cluster

?> You will need to add some scaffolder plugins to accomplish this software template. [Roadie publishes some good scaffolder plugins](https://roadie.io/docs/scaffolder/scaffolder-actions-directory/#roadiehqutilsfswrite)

## Exercise 5

Congratulations you have a very minimal yet powerful platform now. You have a centrally managed Kubernetes cluster that can be developed
and released agnostically. You also have a paved path for app teams to onboard onto your new cluster. Currently getting a change from your
onboarded to your application is somewhat cumbersom. Change the application, merge and create a new release for the application, update
application manifests to use the new version, merge and cut a release for the manifests, then update the deploy repo to point to the new
version of the application manifests. While this approach separates concerns and allows easy rollback and purposeful release it can be
improved. Really we are talking about managing internal dependencies. Way back in chapter two we learned about Dependabot as a way to manage
app dependencies. This is not the only tool that can do this and another popular option is Renovate.

1) [Install Renovate](https://github.com/apps/renovate/new/installations) into your org
2) Create an application change that publishes a new version of your image
3) Configure renovate for the app teams manifest repo. Force Renovate to run from the developer site `https://developer.mend.io/github/<your-org>`
and you should see renovate create a pull request to the app teams manifest repo updating the image version. Hold off on merging this
4) Configure Renovate for the platform deploy repo to watch Kubernetes dependencies.
5) Merge the Renovate PR from the app manifest repo to publish a new version
6) Force a Renovate run on the platform deploy repo and you should see a PR to update the version of the app manifests used.
7) Merge that PR and verify the app changes in your cluster

You now a more streamlined process to get changes into your platform. You can configure renovate to auto merge some changes into the platform.
Say you want all minor and patch versions of application teams to auto merge you can and possibly should configure renovate to do this.

## Exercise 6

Currently you only have a public application onboarded. More typically orgs are deploying private applications.

1) Flip the application repos and container registry to be private and refresh the app in ArgoCD. What happened to the app in ArgoCD?
2) Make the app healthy again.

In order to make the app healthy again you needed to upload some credentials/token. This is now the second time you have had to manually
configure the cluster or ArgoCD so that you can access private resources. This is pretty common but the problem is this approach is
not declarative. If you needed to rebuild the cluster there would be some manual intervention required. We can do better.

1) Install and configure a new platform application for the [External Secrets Operator](https://external-secrets.io/latest/introduction/overview/) (this will use the IRSA you set up in Exercise 3)
2) Upload the PAT that you use in your Argo Credential Template to AWS Secrets Manager
3) Declaratively configure ArgoCD Credential Template leveraging your secret stored in AWS Secrets Manager
4) Add the secrets needed to pull from the private container registry to AWS Secrets Manager
5) Configure Kubernetes to get the ImagePullSecret from External Secrets Operator
