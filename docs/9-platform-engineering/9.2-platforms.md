---
docs/9-platform-engineering/9.2-platforms.md:
  category: Platform Engineering
  estReadingMinutes: 15
  exercises:
    - name: Create a TVP (Thinnest Viable Platform)
      description: |
        Students will use Terraform, Terragrunt and a library/deploy repo structure to create a core EKS platform with ArgoCD and External Secrets Operators
      estMinutes: 3000
      technologies:
        - AWS
        - Terraform
        - Terragrunt
        - Kubernetes
        - ArgoCD
        - ExternalSecrets
---

# Platforms

Backstage is a powerful Internal Developer Platform that excels at software discoverability and accelerates time to value and security adoption through its software templates.
However, Backstage doesn't provide the runtime environment for your applications. In modern software development, we increasingly need to run containerized applications.
Kubernetes has emerged as the de facto standard for container orchestration, but should every developer be expected to be a Kubernetes administrator? The discipline of Platform Engineering addresses this by providing developers
with a centralized platform solution. This manifests as a well-maintained Kubernetes cluster with common tools that streamline the running of applications in a secure, centralized environment.

## TVP

For this exercise, we will create the Thinnest Viable Platform (TVP). This will be an EKS cluster with ArgoCD, Metrics Server installed and configured, AWS LoadBalancer Controller, and ExternalSecrets operator installed and configured.
The end goal of this exercise is to create a simple platform and then onboard a new application to the platform via a Backstage template. This demonstrates a streamlined
approach to getting developers up and running in Kubernetes and provides a framework for Platform teams to build in sensible defaults.

The architecture for this platform will be split into a library and a deploy repo. The library repo will contain Terraform and Argo Application manifests
that install and configure individual components of the platform, each independently versioned. The deploy repo will contain Terragrunt to orchestrate and deploy
your versioned components.

![tvp architecture](./img9/tvp-arch.png ':class=img-center :alt=TVP Architecture')

### Library Repo

This repo will hold versioned Terraform modules and Kubernetes manifests that form the building blocks of our platform. These are split from the deploy repo so that platform teams can develop these components separately without affecting production environments. While ArgoCD will manage most Kubernetes applications, some foundational components (like ArgoCD itself) need to be bootstrapped with Terraform. Each component is independently versioned, allowing teams to update and test components without impacting the entire platform.

Your library repo should contain well-documented, reusable modules and manifests that follow infrastructure-as-code best practices. Teams can reference specific versions of these components, ensuring consistent and reproducible deployments across environments.

```bash
.
├── .github
│   └── workflows
│       ├── pr-lint.yml      # Lint, format and validate Terraform and Kubernetes manifests
│       └── release.yml      # Semantically release modules and applications
├── .gitignore
├── README.md
├── modules
│   ├── argocd
│   │   ├── README.md
│   │   ├── main.tf
│   │   ├── variables.tf
│   │   └── versions.tf
│   ├── eks
│   │   ├── main.tf
│   │   ├── outputs.tf
│   │   ├── variables.tf
│   │   └── versions.tf
│   └── vpc
│       ├── main.tf
│       ├── outputs.tf
│       ├── variables.tf
│       └── versions.tf
├── applications
│   ├── metrics-server
│   │   ├── kustomization.yaml
│   │   └── metrics-server.yaml
│   ├── external-secrets
│   │   ├── kustomization.yaml
│   │   └── external-secrets.yaml
│   └── aws-load-balancer-controller
│       ├── kustomization.yaml
│       └── aws-load-balancer-controller.yaml
```

Your Terraform should be linted, formatted, and validated. Merging to main should create semantic versioned tags for each module in this library repo. For instance,
you should have tags like `eks/v0.1.0` and `metrics-server/v0.0.1`. While specific naming conventions are flexible, the principle is that modules should be versioned
independently. We don't want a change merged to the `vpc` module causing a version bump on the `argocd` module.

?> The [Hashicorp EKS tutorial](https://developer.hashicorp.com/terraform/tutorials/kubernetes/eks) is a good starting point for modeling your modules

### Deploy Repo

This repo orchestrates the deployment of your platform by composing versioned components from the library repo. Using Terragrunt, a lightweight wrapper around Terraform, we manage infrastructure deployments with improved code reuse and reduced configuration complexity. The separation between library and deploy repos creates a clean distinction between defining components and implementing them, allowing platform teams to maintain consistent standards while staying flexible.

The deploy repo structure separates infrastructure from applications, making it clear where and how components are deployed. Infrastructure is managed through Terragrunt, while applications are deployed using Kustomize configurations that reference specific versions from the library repo. This approach provides a streamlined path for both platform engineers managing infrastructure and application teams deploying workloads.

```bash
.
├── .github
│   └── workflows
│       ├── apply.yaml
│       └── pr.yaml
├── .gitignore
├── README.md
├── bootstrap # Folder for terraform that will create required roles, policies, and identity providers needed by CI (CI should ignore this directory)
│   ├── README.md
│   ├── main.tf
│   ├── outputs.tf
│   └── variables.tf
├── common-inputs.yaml # Place for common variables like aws_region, which can be used in all terraform/terragrunt configuration
├── prod
│   ├── infra
│   │   ├── eks
│   │   │   └── terragrunt.hcl
│   │   ├── vpc
│   │   │   └── terragrunt.hcl
│   │   └── argocd
│   │       └── terragrunt.hcl
│   └── applications
│       ├── metrics-server
│       │   └── application.yaml  # Argo Application referencing the deploy repo
│       │   └── kustomization.yaml  # References metrics-server from library repo
│       └── hello-world
│           └── kustomization.yaml  # References app team's repository
│           └── application.yaml  # Argo Application referencing the deploy repo
└── terragrunt.hcl
```

## Exercise 1

Before we can onboard an application we need to create our TVP. We will start by setting up our repos and building out a
repeatable EKS cluster.
- Create the deploy repo:
  - Locally create the bootstrap infra required to have an OIDC ready role for managing infra in your AWS account.
  - Create GitHub Actions for PR plan and Apply of environment infrastructure.
- Create the library repo:
  - Create GitHub Actions that will lint, format, and enforce conventional commits. Merges to main should
  create versioned tags of your modules.
  - Create a VPC module
  - Create an EKS module
- Using Terragrunt orchestrate and deploy your VPC and EKS module in the deploy repo
- Verify that everyone can configure their kubeconfig and authenticate to the EKS module

?> Make both the deploy and the library repo private. This is just out of an abundance of caution as you will have workflows that will likely have very elevated permissions.

## Exercise 2

Wonderful now you have an EKS cluster stood up with IaC. Now we are going to add ArgoCD to the cluster as our preferred method for deploying Kubernetes applications.
First off what is ArgoCD? ArgoCD is a GitOps controller that manages Kubernetes resources by treating Git as the single source of truth. Unlike traditional CI/CD push-based deployments,
ArgoCD uses a pull-based model where it runs inside your cluster and continuously monitors your Git repositories for changes. When it detects drift between the desired state
in Git and the actual state in the cluster, it automatically reconciles the differences. This approach offers several key advantages: enhanced security since no external
system needs direct cluster access, better auditability as all changes are tracked in Git, and improved reliability through constant state reconciliation.
ArgoCD also provides a powerful UI for visualizing your applications' deployment status and health, supports multiple config management tools like Helm and Kustomize,
and can manage applications across multiple clusters from a single interface. Its declarative nature aligns perfectly with Kubernetes' own paradigms, making it a natural fit for cloud-native deployments.

- Create and install ArgoCD into the cluster through a Terrafrom module (write this in the Library repo)
- Actually Deploy and ArgoCD into the cluster via the Deploy repo with Terragrunt
- Verify that both of you can access the ArgoCD UI by portforwarding the `argocd-service`

?> Default admin password is stored as a Kubernetes secret on the cluster

## Exercise 3

Now that we have stood up ArgoCD we can let Argo manage our Kubernetes applications. There are many core applications that all
application teams are going to want to have access. These applications generally fall to the platform teams. Some examples of these are:
- Metrics Server
- OTEL Operators & Collectors
- AWS Load Balancer Controller
- Cert-Manager
- ExternalDNS
- Kyverno/OPA Gatekeeper
- External Secrets Operator
- Karpenter
etc

The list is long but the thing to note here is that these are all Kubernetes applications that are deployed ultimately
via manifest files (Helm charts just result in manifest files). So given our cluster now has a Kubernetes GitOps Controller
installed and configured we are going to let ArgoCD own the deployment of these Kubernetes applications.

- In your library repo create a new directory `applications/metrics-server`
  - Using what you have learned add file(s) that will source manifests from a pinned version of the [official repo for metrics-server](https://github.com/kubernetes-sigs/metrics-server)
  > Verify that pushing the metrics-server application to your library repo generates a tagged version in the library repo
- In your deploy repo create a new directory `applications/metrics-server` under your environment (ie: `prod/applications/metrics-server`)
- Use Kustomize to reference your versioned application in the library repo
- Next create an Argo Application manifest that will point to your deploy repo at `applications/metrics-server`
- Add a new workflow to your deploy repo that will apply the Argo Application manifests
- Verify in the Argo UI that metrics-server was picked up, synced, and healthy.
- Try changing the version of metrics server that you have pinned in your library repo and roll that change out the the cluster.

?> [ArgoCD's Credential Templates](https://argo-cd.readthedocs.io/en/stable/user-guide/private-repositories/#credential-templates) are an easy way to authenticate to private/internal repos

#### UNCHARTED TERRITORY

## Exercise 4

We have the framework for a platform now we need a way for teams to use our platform. In this exercise we will create a backstage template that
injects a new Argo Application into our Deploy repo that references an external manifests repo.
- Create a simple Hello World NodeJS application. Containerize it and publish the resulting image to a contrainer registry
- Create a manifest repo that contains only the required k8s manifests to run your hello world application (likely just a Deployment and a Service)
- Create a Backstage template that will take in a reference to a manifest repo and generate an Argo Application manifest that is injected into
the deploy repo's application folder via a Pull Request.
- Review and merge the pull request
- Verify the application was successfully deployed to the cluster
- Update the application and verify that ArgoCD picked up the change and deployed it automatically
