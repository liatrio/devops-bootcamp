---
docs/9-platform-engineering/9.2-platforms.md:
  category: Platform Engineering
  estReadingMinutes: 15
  exercises:
    - name: Create a TVP (Thinnest Viable Platform)
      description: |
        Students will use Terraform, Terragrunt and a library/deploy repo structure to create a core EKS platform with ArgoCD and External Secrets Operators
      estMinutes: 3000
      technologies:
        - AWS
        - Terraform
        - Terragrunt
        - Kubernetes
        - ArgoCD
        - ExternalSecrets
---

# Platforms

Backstage is a powerful Internal Developer Platform that excels at software discoverability and accelerates time to value and security adoption through its software templates.
However, Backstage doesn't provide the runtime environment for your applications. In modern software development, we increasingly need to run containerized applications.
Kubernetes has emerged as the de facto standard for container orchestration, but should every developer be expected to be a Kubernetes administrator? The discipline of Platform Engineering addresses this by providing developers
with a centralized platform solution. This manifests as a well-maintained Kubernetes cluster with common tools that streamline the running of applications in a secure, centralized environment.

## TVP

For this exercise, we will create the Thinnest Viable Platform (TVP). This will be an EKS cluster with ArgoCD, Metrics Server installed and configured, AWS LoadBalancer Controller, and ExternalSecrets operator installed and configured.
The end goal of this exercise is to create a simple platform and then onboard a new application to the platform via a Backstage template. This demonstrates a streamlined
approach to getting developers up and running in Kubernetes and provides a framework for Platform teams to build in sensible defaults.

The architecture for this platform will be split into a library and a deploy repo. The library repo will contain Terraform and Argo Application manifests
that install and configure individual components of the platform, each independently versioned. The deploy repo will contain Terragrunt to orchestrate and deploy
your versioned components.

![tvp architecture](./img9/tvp-arch.png ':class=img-center :alt=TVP Architecture')

### Library Repo
This repo will hold versioned Terraform modules and Kubernetes manifests that form the building blocks of our platform. These are split from the deploy repo so that platform teams can develop these components separately without affecting production environments. While ArgoCD will manage most Kubernetes applications, some foundational components (like ArgoCD itself) need to be bootstrapped with Terraform. Each component is independently versioned, allowing teams to update and test components without impacting the entire platform.

Your library repo should contain well-documented, reusable modules and manifests that follow infrastructure-as-code best practices. Teams can reference specific versions of these components, ensuring consistent and reproducible deployments across environments.

```bash
.
├── .github
│   └── workflows
│       ├── pr-lint.yml      # Lint, format and validate Terraform and Kubernetes manifests
│       └── release.yml      # Semantically release modules and applications
├── .gitignore
├── README.md
├── modules
│   ├── argocd
│   │   ├── README.md
│   │   ├── main.tf
│   │   ├── variables.tf
│   │   └── versions.tf
│   ├── eks
│   │   ├── main.tf
│   │   ├── outputs.tf
│   │   ├── variables.tf
│   │   └── versions.tf
│   └── vpc
│       ├── main.tf
│       ├── outputs.tf
│       ├── variables.tf
│       └── versions.tf
├── applications
│   ├── metrics-server
│   │   ├── kustomization.yaml
│   │   └── metrics-server.yaml
│   ├── external-secrets
│   │   ├── kustomization.yaml
│   │   └── external-secrets.yaml
│   └── aws-load-balancer-controller
│       ├── kustomization.yaml
│       └── aws-load-balancer-controller.yaml
```

Your Terraform should be linted, formatted, and validated. Merging to main should create semantic versioned tags for each module in this library repo. For instance,
you should have tags like `eks/v0.1.0` and `metrics-server/v0.0.1`. While specific naming conventions are flexible, the principle is that modules should be versioned
independently. We don't want a change merged to the `vpc` module causing a version bump on the `argocd` module.

?> The Hashicorp EKS tutorial is a good starting point for modeling your modules: https://developer.hashicorp.com/terraform/tutorials/kubernetes/eks

### Deploy Repo
This repo orchestrates the deployment of your platform by composing versioned components from the library repo. Using Terragrunt, a lightweight wrapper around Terraform, we manage infrastructure deployments with improved code reuse and reduced configuration complexity. The separation between library and deploy repos creates a clean distinction between defining components and implementing them, allowing platform teams to maintain consistent standards while staying flexible.

The deploy repo structure separates infrastructure from applications, making it clear where and how components are deployed. Infrastructure is managed through Terragrunt, while applications are deployed using Kustomize configurations that reference specific versions from the library repo. This approach provides a streamlined path for both platform engineers managing infrastructure and application teams deploying workloads.

```bash
.
├── .github
│   └── workflows
│       ├── apply.yaml
│       └── pr.yaml
├── .gitignore
├── README.md
├── bootstrap # Folder for terraform that will create required roles, policies, and identity providers needed by CI (CI should ignore this directory)
│   ├── README.md
│   ├── main.tf
│   ├── outputs.tf
│   └── variables.tf
├── common-inputs.yaml # Place for common variables like aws_region, which can be used in all terraform/terragrunt configuration
├── prod
│   ├── infra
│   │   ├── eks
│   │   │   └── terragrunt.hcl
│   │   ├── vpc
│   │   │   └── terragrunt.hcl
│   │   └── argocd
│   │       └── terragrunt.hcl
│   └── applications
│       ├── metrics-server
│       │   └── kustomization.yaml  # References metrics-server from library repo
│       └── hello-world
│           └── kustomization.yaml  # References app team's repository
└── terragrunt.hcl
```

## Exercise 1
Before we can onboard an application we need to create our TVP. We will start by setting up our repos and building out a
repeatable EKS cluster.
- Create the deploy repo:
  - Locally create the bootstrap infra required to have an OIDC ready role for managing infra in your AWS account.
  - Create GitHub Actions for PR plan and Apply of environment infrastructure.
- Create the library repo:
  - Create GitHub Actions that will lint, format, and enforce conventional commits. Merges to main should
  create versioned tags of your modules.
  - Create a VPC module
  - Create an EKS module
- Using Terragrunt orchestrate and deploy your VPC and EKS module in the deploy repo
- Verify that everyone can configure their kubeconfig and authenticate to the EKS module

## Exercise 2
With a working EKS cluster we need to configure ArgoCD on the cluster.
- Create and install ArgoCD into the cluster through a Terrafrom module (write this in the Library repo)
- Actually Deploy and ArgoCD into the cluster via the Deploy repo with Terragrunt
- Verify that both of you can access the ArgoCD UI by portforwarding the `argocd-service`

#### UNCHARTED TERRITORY
## Exercise 3
Now that we have stood up ArgoCD we can let argo manage our k8s applications. There are many core applications that all
application teams are going to want to have access to. Some examples of these are:
- Metrics Server
- OTEL Operators & Collectors
- AWS Load Balancer Controller
- Cert-Manager
- ExternalDNS
- Kyverno/OPA Gatekeeper
- External Secrets Operator
- Karpenter
etc

The list is long but the thing to note here is that these are all Kubernetes applications that are deployed ultimately
via manifest files (Helm charts just result in manifest files). So given our cluster now has a configured Kubernetes GitOps Controller
installed and configured we are going to let ArgoCD own the deployment of these Kubernetes applications.

- In your library repo create a new directory `applications/metrics-server`
- Create an Argo Application (CR you get with Argo) that for metrics-server
- Update your CI workflows to version your k8s applications
- Assuming you are not authoring your metrics-server manifests (pulling from remote, helm chart, whatever). Make sure you add some convienet
way for platform engineers to get the new versions (Makefile?)
- In your deploy repo through a kustomization.yaml reference your versioned Argo Application
- Update CD in your Deploy repo to release the Argo Application to your cluster
- Verify this works

## Exercise 4
We have the framework for a platform now we need a way for teams to use our platform. In this exercise we will create a backstage template that
injects a new Argo Application into our Deploy repo that references an external manifests repo.
- Create a simple Hello World NodeJS application. Containerize it and publish the resulting image to a contrainer registry
- Create a manifest repo that contains only the required k8s manifests to run your hello world application (likely just a Deployment and a Service)
- Create a Backstage template that will take in a reference to a manifest repo and generate an Argo Application manifest that is injected into
the deploy repo's application folder via a Pull Request.
- Review and merge the pull request
- Verify the application was successfully deployed to the cluster
- Update the application and verify that ArgoCD picked up the change and deployed it automatically
