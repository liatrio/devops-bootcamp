---
docs/3-virtual-machines-containers/3.4-containers.md:
  category: Containerization
  estReadingMinutes: 20
  exercises:
    - name: Hello Containers
      description: Complete Docker's 'Hello World'
      estMinutes: 30
      technologies:
        - Docker
    - name: Self-hosted GitHub Action to Nexus containerized
      description: Containerize a self-hosted GitHub Action and Nexus and build a pipeline that pushes a PetClinic build artifact from one container to the other.
      estMinutes: 150
      technologies:
        - Docker
        - GitHub Actions
        - Nexus OSS
---

# Containers

> &ldquo;Containers and virtual machines have similar resource isolation and allocation benefits, but function differently because containers virtualize the operating system instead of hardware. Containers are more portable and efficient.&rdquo;
>
> _- [What is a Container?](https://www.docker.com/resources/what-container)_

![containers image](img3/containers_light.svg ":size=150x150 :class=light-mode-icon :alt= containers image; light mode")
![containers image](img3/containers_dark.svg ":size=150x150 :class=dark-mode-icon :alt= containers image; light mode")

Containers allow you to easily package an application's code, configurations, and dependencies into easy to use building blocks. Containers can help ensure that applications deploy quickly, reliably, and consistently regardless of the deployment environment. This enables the following benefits:

- **Developer Productivity.** Containers enable engineers to run newly developed applications and services in the same infrastructure that their applications will run on in production. This creates a faster feedback loop for engineers about application changes.

- **Environmental Consistency & Versioning.** Container images are a point in time capture of an application's code, configuration, and dependencies. These images are able to be versioned per build, and each artifact is considered immutable. Deployments are represented as code, tagged, and versioned for auditability and clarity.

- **Operational Efficiency.** Being able to build, test, ship, and run the exact same container idempotently through all stages of the Software Delivery Life Cycle makes delivering a high quality, reliable application considerably easier. Continuous Integration servers can run the same container artifact through multiple stages, including integration tests, static code scanning, load testing, and functional testing, all to make sure the artifact passes known quality gates.

- **Resource Density.** Containers maximize the hardware by allowing multiple processes to run on a single piece of hardware or a logical cluster of hardware. Efficiency is a result of the isolation and allocation techniques that managed container services use. This allows for better projected utilization costs, as well as capacity planning for existing and future scaling for traffic needs, and experimentation without costly environment spin-up.

## Containers vs Virtual Machines

![A diagram showing progression from traditional server deployments to virtual machines to container-based deployments](img3/delivery-containers-evolution_light.svg ":class=light-mode-img-center")
![A diagram showing progression from traditional server deployments to virtual machines to container-based deployments](img3/delivery-containers-evolution_dark.svg ":class=dark-mode-img-center")

The picture above represents the progression that has happened over the last 15 years in the industry for deploying applications. In the beginning, apps were deployed to their own physical hosts, each with their own operating system.

Then, Virtual Machines (VMs) were introduced; this allowed for slightly higher process density per physical host, and some logical grouping of applications to be done.

In the third progression, containers are introduced into the flow. Containers are a smaller form of a VM that only contains the application code and its necessary dependencies to run. This progression of technology has led to greater flexibility and scalability.

## Containers in Production

Some people have the mistaken idea that containers are only for development and testing, but there are thousands of companies running in production with containers on Kubernetes and OpenShift. A container image is meant to be immutable, meaning it will be identical no matter where it is deployed, whether it is in development or production hardware. Every iteration of a container will deploy the same way on top of the orchestrator.

Read more from [Docker: What is a Container](https://www.docker.com/resources/what-container) to get a better understanding of containers and how they compare to virtual hosts.

## Knowledge Check

<div class="quizdown">
  <div id="chapter-3/3.4/containers-quiz.js"></div>
</div>

# Docker

> &ldquo;Docker is a platform to create, run and manage containers and container resources (images, networks and data volumes). It consists of a server, a REST API and a CLI client to interact with the API.&rdquo;

Docker is often used synonymously with containers but is actually a platform for using containers. Similar to how Java has development and runtime environments created by different organizations, there are many [other platforms for containers](https://humalect.com/blog/alternatives-to-docker) however Docker is by far the most widely adopted container platform.

![docker image](img3/cloud_docker_light.svg ":size=100x100 :class=light-mode-icon :alt= docker image; light mode")
![docker image](img3/cloud_docker_dark.svg ":size=100x100 :class=dark-mode-icon :alt= docker image; light mode")

## Images and Containers

> &ldquo;An image is an executable package that includes everything needed to run an application--the code, a runtime, libraries, environment variables, and configuration files.&rdquo;
>
> _- [Docker Concepts](https://docs.docker.com/get-started/#docker-concepts)_

> &ldquo;A container is a runtime instance of an image--what the image becomes in memory when executed (that is, an image with state, or a user process).&rdquo;
>
> _- [Docker Concepts](https://docs.docker.com/get-started/#docker-concepts)_

Images are the building blocks of Docker. They define what is in a container. They can be used by other images to extend their functionality. Images can also be shared across hosts.

## Running Containers

Install [Docker](https://docs.docker.com/install) and run the [hello world container](https://hub.docker.com/_/hello-world) `docker run hello-world`

This deceptively simple command actually does a lot which is explained by the output of the container.

If the Docker image hello-world does not exist in the local image repository it downloads it from Docker Hub and adds it to the local repository. You can use `docker image ls` to list the locally installed images. You will see the hello-world image in this list. You can download or update an image directly with `docker pull <IMAGE NAME>`.

Once the hello-world image is available it creates and runs a container from the image. Images define the default command that is run when the container is started. When the command exits the container stops. You can list running and stopped containers with `docker container ls -a`.

Note that it has automatically generated a name for the container since we did not specify one. What happens to the images and containers if you run `docker run hello-world` again?

### Cleaning Up

Remove the stopped hello world containers with `docker rm CONTAINER_ID|NAME`.

Remove the hello world image with `docker image rm IMAGE_ID|NAME`.

?>Docker Desktop on OSX limits the amount of resources containers can use by default. If you experience performance issues with resource intensive containers, make sure to increase the resources available. `Docker Desktop icon` -> `Preferences` -> `Advanced`

## Building Images

Docker images are built by defining a set of steps to create the image in a [Dockerfile](https://docs.docker.com/engine/reference/builder/). Each step begins with a single word instruction, which are uppercase by convention, followed by arguments all on the same line. The image is then built by running `docker build PATH_TO_DOCKERFILE`.

## Base Images

A base image is the starting point from which any new image is created. It is referenced using the `FROM` instruction in a Dockerfile. A proper implementation of base images improves the maintainability of your images. Your base image can be an [official Docker image](https://docs.docker.com/docker-hub/official_images/), such as CentOS, or you can modify an official Docker image to suit your needs, or you can create your own base image from [scratch](https://hub.docker.com/_/scratch). When building images, there are a few things to consider in regards to the base image from which to build from.

### Use official images

Where possible, use [official images](https://hub.docker.com/search?q=&type=image&image_filter=official) from [Docker Hub](https://hub.docker.com/) rather than installing tools manually. For example, the following Dockerfile is installing Java.

```docker
## Don't do this
FROM centos:centos6
RUN yum install java-1.7.0-openjdk-devel -y

## Better
FROM openjdk
```

### Tag specific base image versions

If you omit the tag on the `FROM` line, you will end up with the latest tag. This can result in inconsistencies in the image that is built as the latest tag can change between times when the image is built. Therefore, be sure to always include a tag to pin to a specific upstream image.

```docker
## Don't do this
FROM openjdk

## Better
FROM openjdk:7
```

### Use specialized images

Container images, especially base images, should be specialized for a specific purpose and not contain any unnecessary packages or tools. Even small tools that you might normally consider essential or innocuous such as text editors, debugging tools or network diagnostic tools should be avoided. Adding unnecessary tools not only increases the size of the image, but also the attack surface of a container which could introduce a security vulnerability.

### Images require security updates

All Docker images, even a specialized image without any unnecessary packages or tools, still requires security updates. Security updates should be part of the development process. There are tools, like [dependabot](https://github.com/dependabot) to make this easier.

## Layers

A Docker image consists of read-only layers each of which represents a Dockerfile instruction. The layers are stacked and each one is a delta of the changes from the previous layer. Care must be taken when designing a Dockerfile to optimize both the time it takes to build the image as well as the size of the image that is created.

?>Layers reduce the time it takes to build the image through efficient use of caching.

### Order of instructions

Docker will cache layers from previous builds to decrease the time required to build images. However, once Docker detects that a layer needs to be rebuilt, then all layers after it must also be rebuilt. Consider the following Dockerfile.

```docker
FROM ubuntu
RUN apt-get update
RUN apt-get install -y openjdk-8-jdk
COPY . /app
```

### Dependency layer

Downloading dependencies for an application can be an expensive operation. You can leverage Docker layer caching to avoid this in image build time by having a separate layer that resolves the dependencies for the application. In the following example, we have a layer to `RUN gradle build` before we add the source code to the image. This results in a layer with all our application dependencies downloaded.

```docker
COPY build.gradle gradle.properties settings.gradle
RUN gradle build
```

## Multi-stage builds

One of the most challenging things about building images is keeping the image size down. Each instruction in the Dockerfile adds a layer to the image, and you need to remember to clean up any artifacts you don’t need before moving on to the next layer. With multi-stage builds, you use multiple `FROM` statements in your Dockerfile. Each `FROM` instruction can use a different base, and each of them begins a new stage of the build. You can selectively copy artifacts from one stage to another, leaving behind everything you don’t want in the final image.

### Build from source

In order to ensure repeatability of image builds, you should put as much of the build process as possible inside the Dockerfile. For example, if you run Gradle to build a jar from your workspace and then have the Dockerfile copy in the jar file, you run the risk of other developers not having the same tools on their workspace. In order to mitigate that, install and run Gradle from within the Dockerfile:

```docker
COPY build.gradle gradle.properties settings.gradle
RUN gradle build
COPY . /app
RUN gradle clean build release
```

### Separate build stage

The concern with running the build within your Dockerfile is the additional space consumed by the image for all the build tools. This can be addressed through the use of a separate stage for the build.

```docker
## Builder stage
FROM openjdk:7 as builder
COPY build.gradle gradle.properties settings.gradle
RUN gradle clean build release
COPY . /app

## Runner stage
FROM ubuntu as runner
COPY --from=builder /app/dist/web-app-1.0.1.BUILD.war app/webapps/web-app.war
```

## Processes

The final step in the `Dockerfile` is to run the application using a process command. The recommendation is to separate areas of concern by using **one process per container**.

- **Simplicity** - Running multiple processes within a container requires additional scripting within the container to coordinate the startup of each process.
- **Reliability** - When the foreground process for a container exits, the container orchestrator can automatically restart the container. If there is more than one process, and it exits, it is up to you to manage the restart of the process within the container rather than allowing the orchestrator to restart it.
- **Scalability** - If one process requires additional capacity and triggers an auto-scaling event, then all processes within the container are also scaled.

The recommendation is the exec form of the `ENTRYPOINT` instruction to run the one process:

```docker
ENTRYPOINT ["java", "-jar", "/app.jar"]
```

## Docker Hub

[Docker Hub](https://hub.docker.com/) is an online Docker image registry where users can upload their ready-built images, or pull down images uploaded by the Docker community. Similar to Atlas for Vagrant boxes, Docker Hub is driven by its users. If there isn't an image available with the features you want, you can make one and upload it yourself.

## Best Practices

The following best practices are part of a philosophy for containers in order to make them scalable, reusable, and quickly deployable.

- Ephemeral Containers: A container should, as much as possible, be able to be stopped and destroyed, then rebuilt and replaced with an absolute minimum set up and configuration.
- Decouple Applications: A container should have only one concern.

See [Docker: Dockerfile Best Practices](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/) for more best practices regarding performance and optimization.

## Knowledge Check

<div class="quizdown">
  <div id="chapter-3/3.4/docker-quiz.js"></div>
</div>

## Exercise 1

Create a Self-Hosted GitHub Actions Docker image similar to the virtual machine you created using VMWare Fusion.

?> GitHub Actions requires the following:

- An organization name (your GitHub username if you aren't part of an org)
- A personal access token

1. Create a `Dockerfile` in a new folder
2. Add the `FROM` instruction to use [Alpine Linux](https://hub.docker.com/_/alpine) as the base image.
3. Update the packages this container uses
4. Install the `curl` and `jq` packages, as well as any dependencies that are needed to build your project that will use this runner.

?> You will use the `RUN` instruction for most of these steps but you should also familiarize yourself with other Dockerfile instructions.

5. Setup a non-root user to run as, we'll refer to this as `GHA`
6. Download and unzip the latest GitHub actions runner linux release into the `GHA` home folder
7. Create a start shell script that configures and runs the GitHub actions package
8. Set the script from #7 to be included as part of your docker image
9. Make sure that the `GHA` user owns the script from #7 and that it is executable
10. Set the docker image to run as the `GHA` user
11. Set the entry point of the docker image to be the script from #7
12. Build your docker image and tag it as `runner`
13. Run your docker container in detached mode and verify that it was able to connect to GitHub successfully by looking at the logs

?> Make sure to supply your organization name and personal access token as env variables.

## Exercise 2

Create a docker image for your Nexus server similar to the virtual machine you created using VMWare Fusion.

?> Nexus has it's own set up steps to get up and running and most of those are not shared with GitHub Actions

## Knowledge Check

<div class="quizdown">
  <div id="chapter-3/3.4/docker-self-hosted-runner-quiz.js"></div>
</div>

## Deliverables

- Understand the basic concepts of containers.
- Be familiar with the Dockerfile instructions used to build an image.
- Understand the best practices for Docker images and containers.
- Discuss the differences between running `vagrant init ubuntu/bionic64 && vagrant up && vagrant ssh` and `docker run -it ubuntu:bionic bash`
- Discuss the advantages/disadvantages of using containers vs. virtual machines.
- Discuss the differences between containers and virtual machines in regards to security.
- Discuss some possible challenges of using containers in an enterprise production environment.
