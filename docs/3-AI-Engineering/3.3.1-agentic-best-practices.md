---
docs/3-AI-Engineering/3.3.1-agentic-best-practices.md:
  category: AI Engineering
  estReadingMinutes: 30
---

# AI Development for Software Engineers

AI-enhanced software development is quickly becoming the norm, but effectively leveraging these tools requires more than just casual prompting. While it's tempting to ask an AI to "create me an Instagram clone from scratch," the result will likely be an unmanageable mess that fails to meet production standards. Thoughtful, structured use of these tools leads to more maintainable and reliable outcomes. This section provides engineers with advanced techniques for integrating AI into their development workflow while maintaining engineering rigor and oversight, offering a practical framework for getting the most value from these powerful tools.

![No Easy Button](img3/no-easy-button.png ":size=600 :alt=No Easy Button")

## Thoughtful AI Development

Instead of starting with the "Make me an Instagram clone" prompt we still need to start with thoughtful planning. I have found it personally really helpful to prompt the LLM to engage in a conversation that converts my 'back of the napkin' idea into a detailed project specification. A good approach is to brainstorm, generate a plan, iterate on the plan. Leverage all the good development practices you know (small batch, TDD, etc.)

Much of this section was influenced by [Harper Reed's LLM Codegen Workflow](https://harper.blog/2025/02/16/my-llm-codegen-workflow-atm/) and the internal expirementing that we have been doing here at Liatrio. As LLMs and the tools around them continue to improve so will these best practices. The aim here to codify a practical workflow that should hold up over time.

Most of these practices apply to both greenfield and brownfield projects. However it is worth callingout that for brownfield projects the planning phase is slightly modified to be focused on the task at hand.

### 1. Brainstorm Spec

 Use a conversational LLM (not a Chain of Thought model) to iteratively refine an idea into a detailed specification:

* Engage the LLM in a step-by-step, question-based conversation
* Focus on extracting and structuring all relevant details
* Compile findings into a comprehensive specification document

**Example Prompt**:

```text
Ask me one question at a time so we can develop a thorough, step-by-step spec for this idea.

Each question should build on my previous answers, and our end goal is to have a detailed specification I can hand off to a developer. Let's do this iteratively and dig into every relevant detail. Only one question at a time. When giving options, always format them in a numbered list.

Here's the idea: 
[YOUR_IDEA]
```

After completing the brainstorming:

```text
Now that we've wrapped up the brainstorming process, compile our findings into a comprehensive, developer-ready specification. Include all relevant requirements, architecture choices, data handling details, error handling strategies, and a testing plan so a developer can immediately begin implementation.
```

Save this file as `spec.md`.

Here you should practice small batch delivery by committing this spec. In AI assisted development _how_ you generated the content is critical. In your commit message you should include the following:

* Date generated
* LLM used
* Prompt used
* Parameters tuned (e.g. Temperature, Top P/Top K, etc.)
* Notes on any changes or clarifications

### 2. Planning

Take the `spec.md` generated in the previous step and now we will break it down into small, iterative implementation chunks:

* Provide the spec to a reasoning-focused model
* Request a detailed blueprint broken into small steps
* Structure these steps as prompts for a code-generation model

**Example Prompt**:

```text
Draft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. From here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step in a test-driven manner.

<YOUR_SPEC>
```

Save this as something like `prd.md`. Again practice small batch delivery and don't forget prompt details in your commit message.

?> An alternative to using a reasoning model to generate the PRD is to leverage a tools such as [TaskMaster AI](https://www.task-master.dev/). The idea is very similar though this tool adds additional features. I encourage you to give it a spin and compare it to the simplified method. PS it does have an MCP server

### 3. Execution

Apply the prompts from Step 2 to build the project incrementally:

* Set up project repository boilerplate (commit)
* Use the prompts sequentially with a code-generation tool
* Test and verify each piece before moving to the next, committing after each step
* Provide code context back to the LLM when debugging as needed

?> For the iterative development look into your IDEs Agentic capabilities. Leveraging things like instruction/rule files, MCP servers, Webbased docs, and relevant context in prompts can drastically improve the experience. These differ from Agentic IDE but most of the major players have parallel capabilities.

For existing codebases, adapt by:
* Generating a list of required tests or features first
* Grabbing relevant code context
* Implementing specific components one at a time

## üîç Other Practical AI Techniques

These techniques leverage AI's strengths while mitigating its weaknesses through careful engineering practices. They are suitable for both greenfield and brownfield projects.

### 1. The "Second Opinion" Technique

![Cage Match](img3/cage-match.jpg ":size=600 :alt=Cage Match")

Pit multiple LLMs against each other. Task another LLM to review your existing technical work searching for simpler or more idiomatic solutions.

**How to use**:
* Copy your artifact (code, architecture plan, etc.) into an AI chat
* Request feedback focused on simplicity, clarity, or best practices
* Apply insights that genuinely improve your implementation

**Example Prompts**:

```text
What do you think of this implementation? Can you suggest any simpler or more idiomatic ways to achieve the same result?
```

```text
Here is a draft architecture plan. Are there any obvious complexities I've introduced, or simpler patterns I might have overlooked?
```

### 2. The "Throwaway Debugging Scripts" Technique

Take advantage of LLMs ability to generate short, functional scripts to automate debugging steps without requiring codebase integration.

**How to use**:
* Identify specific debugging steps you would perform manually
* Ask the LLM to write a script (10-30 lines) to perform these steps
* Use the script as a disposable tool to diagnose issues

**Example Scenarios**:

```text
Generate a Python script that executes sub-parts of this Elasticsearch query sequentially
to help identify which part is causing unexpected results. The script should:
1. Connect to our Elasticsearch cluster at <ES_ENDPOINT>
2. Execute each major clause of the query independently
3. Print the number of results and a sample document for each test
```

```text
Create a Python script using the `requests` library to test different API endpoints
with various query parameters. The script should:
1. Make GET requests to <BASE_URL>/api/endpoint with different combinations of parameters
2. Log the response status code and response time for each request
3. Save the full response when the status code is not 200
4. Include proper error handling and timeouts
```

You should not spend too many cycles on this and they should be short and crude. The idea is to speed up your local testing not build a new standard way of debugging.

### 3. Plugging Technical Gaps

Quickly get up to speed in unfamiliar domains by using LLMs to generate initial code or configuration snippets.

![I Know Kung Fu](img3/i-know-kung-fu.gif ":class=img-center :size=400 :alt=I Know Kung Fu")

**Example Scenario**:

```text
Generate an nginx configuration block to reverse proxy requests from /api/* to http://backend-service:8080, ensuring that the Host header is preserved.
```

```text
Create a production-ready multi-stage Dockerfile for a Python FastAPI application that includes:
1. A build stage with all build dependencies
2. A final stage using a slim Python image
3. Proper layer caching for faster builds
4. Non-root user for security
5. Health checks
6. Proper signal handling for graceful shutdowns. The app uses Poetry for dependency management and needs to connect to both PostgreSQL and Redis services.
```

**Important**: Always have code generated in unfamiliar areas reviewed by a domain expert.

## üìù Documenting Your Prompts

Given the variability of LLM outputs across models, settings, and versions, documenting your prompts is crucial for effective prompt engineering.

### Why Document?

* Provides a record of what prompts were used and what results were obtained
* Essential for revisiting work and troubleshooting unexpected outputs
* Helps test prompts on new model versions
* Facilitates knowledge sharing across teams

### What to Document

* Prompt text
* Model used (and version)
* Configuration settings (temperature, Top-K/Top-P, etc.)
* Date and context
* Expected vs. actual output
* Evaluation notes

### Best Practices

* Save prompts in version control for operationalized systems
* Use a structured format (e.g., a table in a markdown file)
* Include examples of good and bad outputs
* Document prompt engineering iterations and their results

## üß† Maintaining the "Dumb Tool" Perspective

Always remember that LLMs are statistical models guessing the next token, not intelligent beings with understanding.

### How to Apply

* **Ask Clearly Bounded Questions**: Frame prompts with constrained expected outputs that are easy to verify
* **Remain the Decision-Maker**: Generate possibilities with AI, but retain decision authority
* **Leverage Objectivity**: Get unfiltered feedback based on patterns in training data, not personal opinion

### Practical Example

Instead of:
```text
"Write me the code for feature X"
```

Follow this workflow:
1. Brainstorm the specification first
2. Create a detailed implementation plan
3. Use AI for small, well-defined code generation tasks
4. Review and test each piece before integration

For feedback, be specific:
```text
Provide a thorough code review including line numbers and contextual information. This will be passed to a teammate, so be precise and don't hallucinate.
```

## Knowledge Check

<div class="quizdown">
  <div id="chapter-3/3.3/agentic-best-practices-quiz.js"></div>
</div>

## Deliverables

* Which of these techniques have you used before?
* Have you found any other techniques that you have found helpful?
