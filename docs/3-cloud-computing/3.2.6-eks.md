# EKS

In the previous section, we managed containers using ECS. In this section, we'll continue exploring container management, this time using EKS.

## Intro to Kubernetes

>Kubernetes is a portable, extensible open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. [Kubernetes](https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/#what-kubernetes-and-k8s-mean)

![An overview of Kubernetes cluster components. A developer is shown interacting with the API server, while a user interacts with the Kube-Proxy](./img3/kubernetes-overview.svg ':class=img-center')

| Resource | Description |
|----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Pods | The lowest point of organization for Kubernetes is the pod. A pod is a network wrapper around one or more containers, though, common practice is to use one container per pod. Pods have their own networking layer which allows communication between containers. |
| Nodes | A node is a worker machine in Kubernetes. A node may be a virtual or physical machine depending on the infrastructure. A node can contain a single pod or a collection of multiple pods. Nodes also have their own networking layer which makes pod communication across nodes possible. |
| Services | A service is a REST object similar to a pod that holds all of the networking information for each individual pod. This is because pods are often ephemeral and do not have a static IP address. This makes it so individual pods don't have to worry about the location of every other pod, they can simply route their traffic through a service. |
| Clusters | Clusters are the largest of the organizational structure of Kubernetes. A cluster can contain multiple nodes and has its own networking layer for communication between nodes. |

## Intro to EKS

![An overview of EKS architecture.](./img3/eks.webp ':class=img-center')

Similar to ECS, Amazon's Elastic Container Service for Kubernetes (EKS) allows users to manage containers distributed amongst various AWS instances. However, where ECS is an Amazon specific container orchestration tool, EKS utilizes the popular container management platform Kubernetes to manage containers on AWS.

> Amazon Elastic Container Service for Kubernetes (Amazon EKS) is a managed service that makes it easy for you to run Kubernetes on AWS without needing to stand up or maintain your own Kubernetes control plane. Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. [Amazon EKS](https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html)

Although AWS provides ECS as a container management tool, the popularity of Kubernetes spurred the creation of EKS. Before EKS, Kubernetes was commonly used in conjunction with AWS resources, so Amazon created EKS to make the integration between Kubernetes and AWS seamless. EKS runs the Kubernetes management infrastructure behind the scenes so that the user only has to worry about adding worker nodes to their cluster. Both vanilla Kubernetes and EKS use `kubectl` as a tool to interact with clusters, so experienced users of Kubernetes will find it easy to transition to EKS. 


## Kubernetes Components

Kubernetes does a lot behind the scenes to manage and maintain the various workloads that run on a cluster. At the heart of Kubernetes cluster management are the [Kubernetes components](https://kubernetes.io/docs/concepts/overview/components/) that perform these management tasks.
Once you have set your desired state for your cluster, these components work together to make sure that your cluster replicates and maintains this state.

Kubernetes components can be roughly split into two groups:
1. Control plane components make decisions about the cluster at a global level,
such as storing the state of the cluster or scheduling different workloads.
2. Worker node components are those that need to run on every worker node in
the cluster, such as the agent to start up a Docker container.

#### Control Plane Components

Control plane components make global decisions about the cluster. While control
plane components can be run on any machine in a cluster, they are typically all
run on a single machine on which no other container runs. A machine that
only runs control plane components is called a control plane node.

[kube-api-server](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/) -
The kube-api-server is the component of the control plane that exposes the Kubernetes API.
It acts as the front end of a Kubernetes cluster.

[etcd](https://etcd.io/docs/) - etcd is a key-value store, it acts as data storage
for a Kubernetes cluster.

[kube-scheduler](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/) -
The kube-scheduler is the process that assigns pods to run on particular worker nodes.

[kube-controller-manager](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/) -
The kube-controller-manager is a collection of different controllers each of which
is handling a separate task, such as responding to worker node failure, or managing
replication of pods. Each controller in the controller-manager follows separate
logic, but they run as a single process for simplicity.

#### Worker Node Components

Worker node components are those that run on every worker node in a cluster.
They manage the cluster on a much more limited scope, controlling only the
machine they run on.

[kubelet](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/) -
The kubelet is the primary Kubernetes agent process that runs on each worker node. It
handles tasks such as informing the API server that the node is healthy and
starting containers that are scheduled to run on the node.

[kube-proxy](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/) -
The kube-proxy is the networking agent which runs on every worker node in the cluster.
It maintains networking rules on nodes to allow pods to communicate based on
the configuration of the cluster.

## Exercise - Creating an EKS cluster

In the previous section, we launch the SockShop demo on an ECS cluster to demonstrate deploying a microservice application. This exercise will have you deploy SockShop on EKS so you can explore the differences between the two services.

1. Create IAM roles to allow AWS to manage EKS services
  - You will need a role to allow EKS to manage clusters
  - You will need another role to allow EC2 instances to call EKS services(will need to refer to CloudFormation script from AWS tutorial)
2. Create an EKS cluster
3. Connect your `kubectl` tool to your cluster
4. Create a Launch Template with a Kubernetes optimized AMI. 

?> Make sure to use an EC2 instance type with enough CPU and memory resources for each nodes overhead and the pods workload (t2.micro will not be enough). Look through the different [EC2 instance types](https://aws.amazon.com/ec2/instance-types/)
 
5. Specify your launch template when creating a Node Group to launch Kubernetes EC2 images. Set max nodes to 4, desired nodes to 3, and minimum nodes to 2.
6. Launch SockShop application on your cluster using the `complete-demo.yaml` in the [microservice-demo](https://github.com/liatrio/microservices-demo) repo

?> Depending on the version of Kubernetes you select when creating your EKS cluster, the `complete-demo.yaml` may need to be updated.

## Exercise - Adding an autoscaler to your cluster

Now you will add an autoscaler to your cluster that will automatically scale your Node Group up and down depending on the capacity of the nodes in the group.

1. Read about the [Kubernetes autoscaler](https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html)
2. Using `kubectl` or `k9s` check that there are 3 nodes running in your Node Group
3. Follow the AWS documentation to [add a cluster autoscaler](https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html) to the cluster you created in the previous exercise.
  
  ?> Pay particular attention to the policy you are adding to your Node Groups IAM Role

4. Verify that there is a `cluster-autoscaler` running in your `kube-system` namespace. Check the logs to see what it is doing.
5. Check that the autoscaler successfully scales your Node Group down to 2 instances.

## Exercise - Adding an Ingress controller to your cluster
Finally we will add an Ingress controller so that you can access this cluster without having to use port-forwardding.
1. Edit the `complete-demo.yaml` file so the front-end service is not outward facing and reapply the configuration.
2. Install an Ingress controller that works with AWS.
3. Create an ingress resource to talk to the ingress controller with the name `sock-shop-ingress` inside the sock-shop namespace.

  ?> Make sure to add any annotations that are needed for the ingress controller 

5. Run `kubectl get ingress -n sock-shop` in order to get the address of your ingress.
6. Make sure that your cluster works the same way that it would when you were port-forwarding it.

## Deliverables

- Discuss the differences between ECS and EKS
- Explain how services remedy the difficulties of ephemeral pods
- Describe the benefits/drawbacks of using EKS in enterprise
- Discuss why adding an IAM role policy in the autoscaler exercise might be a bad idea, and why [fine-grained IAM roles](https://aws.amazon.com/blogs/opensource/introducing-fine-grained-iam-roles-service-accounts/) are better.
