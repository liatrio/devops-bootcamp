# EKS

In the previous section, we managed containers using ECS. In this section, we'll continue exploring container management, this time using EKS.

## Intro to Kubernetes

>Kubernetes is a portable, extensible open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. [Kubernetes](https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/#what-kubernetes-and-k8s-mean)

![An overview of Kubernetes cluster components. A developer is shown interacting with the API server, while a user interacts with the Kube-Proxy](./img3/kubernetes-overview.svg ':class=img-center')

| Resource | Description |
|----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Pods | The lowest point of organization for Kubernetes is the pod. A pod is a network wrapper around one or more containers, though, common practice is to use one container per pod. Pods have their own networking layer which allows communication between containers. |
| Nodes | A node is a worker machine in Kubernetes. A node may be a virtual or physical machine depending on the infrastructure. A node can contain a single pod or a collection of multiple pods. Nodes also have their own networking layer which makes pod communication across nodes possible. |
| Services | A service is a REST object similar to a pod that holds all of the networking information for each individual pod. This is because pods are often ephemeral and do not have a static IP address. This makes it so individual pods don't have to worry about the location of every other pod, they can simply route their traffic through a service. |
| Clusters | Clusters are the largest of the organizational structure of Kubernetes. A cluster can contain multiple nodes and has its own networking layer for communication between nodes. |

## Intro to EKS

![An overview of EKS architecture.](./img3/eks.webp ':class=img-center')

Similar to ECS, Amazon's Elastic Container Service for Kubernetes (EKS) allows users to manage containers distributed amongst various AWS instances. However, where ECS is an Amazon-specific container orchestration tool, EKS utilizes the popular container management platform Kubernetes to manage containers on AWS.

> Amazon Elastic Container Service for Kubernetes (Amazon EKS) is a managed service that makes it easy for you to run Kubernetes on AWS without needing to stand up or maintain your own Kubernetes control plane. Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. [Amazon EKS](https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html)

Although AWS provides ECS as a container management tool, the popularity of Kubernetes spurred the creation of EKS. Before EKS, Kubernetes was commonly used in conjunction with AWS resources, so Amazon created EKS to make the integration between Kubernetes and AWS seamless. EKS runs the Kubernetes management infrastructure behind the scenes so that the user only has to worry about adding worker nodes to their cluster. Both vanilla Kubernetes and EKS use `kubectl` as a tool to interact with clusters, so experienced users of Kubernetes will find it easy to transition to EKS. 


## Kubernetes Components

Kubernetes does a lot behind the scenes to manage and maintain the various workloads that run on a cluster. At the heart of Kubernetes cluster management are the [Kubernetes components](https://kubernetes.io/docs/concepts/overview/components/) that perform these management tasks.
Once you have set your desired state for your cluster, these components work together to make sure that your cluster replicates and maintains this state.

Kubernetes components can be roughly split into two groups:
1. Control plane components make decisions about the cluster at a global level,
such as storing the state of the cluster or scheduling different workloads.
2. Worker node components are those that need to run on every worker node in
the cluster, such as the agent to start up a Docker container.

#### Control Plane Components

Control plane components make global decisions about the cluster. While control
plane components can be run on any machine in a cluster, they are typically all
run on a single machine on which no other container runs. A machine that
only runs control plane components is called a control plane node.

[kube-api-server](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/) -
The kube-api-server is the component of the control plane that exposes the Kubernetes API.
It acts as the front end of a Kubernetes cluster.

[etcd](https://etcd.io/docs/) - etcd is a key-value store, it acts as data storage
for a Kubernetes cluster.

[kube-scheduler](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/) -
The kube-scheduler is the process that assigns pods to run on particular worker nodes.

[kube-controller-manager](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/) -
The kube-controller-manager is a collection of different controllers each of which
is handling a separate task, such as responding to worker node failure, or managing
replication of pods. Each controller in the controller-manager follows separate
logic, but they run as a single process for simplicity.

#### Worker Node Components

Worker node components are those that run on every worker node in a cluster.
They manage the cluster on a much more limited scope, controlling only the
machine they run on.

[kubelet](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/) -
The kubelet is the primary Kubernetes agent process that runs on each worker node. It
handles tasks such as informing the API server that the node is healthy and
starting containers that are scheduled to run on the node.

[kube-proxy](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/) -
The kube-proxy is the networking agent which runs on every worker node in the cluster.
It maintains networking rules on nodes to allow pods to communicate based on
the configuration of the cluster.

#### Service Types

![](img3/service-types-relationship.svg ':size=300px :class=img-center')

| Type | Description |
|----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ClusterIP | The default service type in Kubernetes. It exposes a service using an internal-cluster IP; no external access is configured. |
| NodePort | Exposes the service on each node's IP in a cluster using a single, static port (the NodePort). NodePort services can be contacted outside of the cluster using `<NodeIP>:<NodePort>`. A ClusterIP Service, to which the NodePort Service routes, is automatically created. |
| LoadBalancer | Exposes a service using a cloud provider's load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created. |

## Exercise - Creating an EKS cluster

In the previous section, we launch the SockShop demo on an ECS cluster to demonstrate deploying a microservice application. This exercise will have you deploy SockShop on EKS so you can explore the differences between the two services.

1. Create IAM roles to allow AWS to manage EKS services
  - You will need a role to allow EKS to manage clusters
  - You will need another role to allow EC2 instances to call EKS services(will need to refer to CloudFormation script from AWS tutorial)
2. Create an EKS cluster. Ensure you've created a security group that permits all traffic from your IP only.
3. Connect your `kubectl` tool to your cluster
4. Create a Launch Template with a Kubernetes optimized AMI. 

?> Make sure to use an EC2 instance type with enough CPU and memory resources for each node's overhead and the pod's workload (t2.micro will not be enough). Look through the different [EC2 instance types](https://aws.amazon.com/ec2/instance-types/)
 
5. Specify your launch template when creating a Node Group to launch Kubernetes EC2 images. Set max nodes to 4, desired nodes to 3, and minimum nodes to 2.
6. Launch SockShop application on your cluster using the `complete-demo.yaml` in the [microservice-demo](https://github.com/liatrio/microservices-demo) repo

?> Depending on the version of Kubernetes you select when creating your EKS cluster, the `complete-demo.yaml` may need to be updated.

## Exercise - Adding an autoscaler to your cluster

Now you will add an autoscaler to your cluster that will automatically scale your Node Group up and down depending on the capacity of the nodes in the group.

1. Read about the [Kubernetes autoscaler](https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html)
2. Using `kubectl` or `k9s` check that there are 3 nodes running in your Node Group
3. Follow the AWS documentation to [add a cluster autoscaler](https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html) to the cluster you created in the previous exercise.
  
  ?> Pay particular attention to the policy you are adding to your Node Groups IAM Role

4. Verify that there is a `cluster-autoscaler` running in your `kube-system` namespace. Check the logs to see what it is doing.
5. Check that the autoscaler successfully scales your Node Group down to 2 instances.

## Exercise - Create a ClusterIP Service (AWS Cluster)

1. Create a [namespace](https://kubernetes.io/docs/tasks/administer-cluster/namespaces/#creating-a-new-namespace) using the format `NAME-bc-svc`.

1. Create and apply a [pod](https://kubernetes.io/docs/concepts/workloads/pods/) named `jenkins` using the image `jenkins/jenkins` (in `NAME-bc-svc` namespace). The name of the container will also be `jenkins`. Add a container port for `8080`.  Add a label with the key `app` and the value `jenkins`.

2. Create a [service](https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service) of type ClusterIP. The selector and label should match the label key/value pair used for the pod. The port/targetPort should be `8080` and the protocol `TCP`.

3. Jenkins is not accessible outside the cluster with this service type. To access it on localhost, port forward the service.

4. Delete only the ClusterIP service.

## Exercise - Create a NodePort Service for (AWS Cluster)

1. Use the same namespace and pod from the ClusterIP exercise.

2. Create a service of type NodePort. Follow the same guidelines as the previous service, but add a nodePort for `32000`.

3. Jenkins is now accessible without port forwarding at `http://NODE_ID:32000`.
  - There needs to be a security group in place for the node to open up port 32000.

4. Delete only the NodePort service.

## Exercise - Create a LoadBalancer Service for (AWS Cluster)

1. Use the same namespace and pod from the ClusterIP exercise.

2. Create a service of type LoadBalancer. Follow the same guidelines as the previous service, remove the nodePort. Use either the AWS CLI or console to verify that a load balancer was created.

3. Jenkins is now accessible outside the cluster using the LoadBalancer service and load balancer hostname.

4. Delete your `NAME-bc-svc` namespace when finished.

## Ingresses

Ingresses are a Kubernetes resource that create an entry point for network traffic to be routed from outside our cluster to pods running in the cluster, similar to a Service of type LoadBalancer but more configurable. The specifics of how traffic is routed into the cluster depend on the network the cluster is running in and is therefore heavily dependent on the platform. For our EKS cluster, AWS uses Application or Network Load Balancers. The Load Balancers are created and configured automatically for us by the AWS Load Balancer Controller when we create an Ingress resource. Read more about [Application load balancing on Amazon EKS](https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html) and the [AWS Load Balancer Controller](https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/).

?> The AWS Load Balancer Controller is automatically provisioned when we create an EKS cluster so we don't need to install it.

In order for an Ingress to route traffic from our network load balancer to our pods, we also need an Ingress Controller. Read more about [Ingresses](https://kubernetes.io/docs/concepts/services-networking/ingress/) and [Ingress Controllers](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/) 

## Exercise - Adding an Ingress and Ingress Controller to your cluster

We will add an Ingress controller so that you can access this cluster without having to use port-forwarding.
1. Install the NGINX Ingress controller. There are different controllers to choose from, but the NGINX Ingress controller works with AWS. You can learn more [here](https://kubernetes.github.io/ingress-nginx/).
2. Create an Ingress resource to talk to the NGINX Ingress controller with the name `sock-shop-ingress` inside the sock-shop namespace.

  ?> Make sure to include the `ingressClassName` if you're using a K8s version ≥ 1.19.

3. Run `kubectl get Ingress -n sock-shop` in order to get the address of your Ingress.
4. Make sure that your cluster works the same way that it would when you were port-forwarding it.

## Deliverables

- Discuss the differences between ECS and EKS
- Explain how services remedy the difficulties of ephemeral pods
- Describe the benefits/drawbacks of using EKS in enterprise
- Discuss why adding an IAM role policy in the autoscaler exercise might be a bad idea, and why [fine-grained IAM roles](https://aws.amazon.com/blogs/opensource/introducing-fine-grained-iam-roles-service-accounts/) are better.
- Discuss how Ingresses are different from Services
